{"title": "Kauri: Scalable BFT Consensus with Pipelined Tree-Based Dissemination and Aggregation", "abstract": "With the growing commercial interest in blockchains, permissioned implementations have received increasing attention. Unfortunately, the BFT consensus algorithms that are the backbone of most of these blockchains scale poorly and ofer limited throughput. Many state-of-the-art algorithms require a single leader process to receive and validate votes from a quorum of processes and then broadcast the result, which is inherently non-scalable. Recent approaches avoid this bottleneck by using dissemination/aggregation trees to propagate values and collect and validate votes. However, the use of trees increases the round latency, which ultimately limits the throughput for deeper trees. In this paper we propose Kauri, a BFT communication abstraction that can sustain high throughput as the system size grows, leveraging a novel pipelining technique to perform scalable dissemination and aggregation on trees. Our evaluation shows that Kauri outperforms the throughput of state-of-the-art permissioned blockchain protocols, such as HotStuf, by up to 28x. Interestingly, in many scenarios, the parallelization provided by Kauri can also decrease the latency. CCS Concepts: ? Computer systems organization ? Reliability; Fault-tolerant network topologies.</p>", "-": "\n        The increasing popularity of blockchains in addressing an\nexpanding set of use cases, from enterprise to governmental\napplications [\n        \n        ], led to a growing interest in permissioned\nblockchains, such as Hyperledger Fabric [\n        \n        ]. In contrast to\ntheir permissionless counterparts, permissioned blockchains\ncan ensure deterministic transaction finality, which is a key\nrequirement in many settings [\n        \n        ], and can ofer high\nthroughput in small sized systems [\n        \n        ].\n      \n        However, emerging use cases for permissioned blockchains\nrequire the system to scale to hundreds of participants [\n        \n        ].\nFor instance, the Diem blockchain states that: ?Our goal was\nto choose a protocol that would initially support at least 100\nvalidators and would be able to evolve over time to support\n500?1,000 validators\" [\n        \n        ]. In addition to that, a recent paper\nfrom IBM [\n        \n        ] discusses the need to extend HyperLedger to\nsupport deployments above 100 nodes in order to address\nthe requirements of platforms such as Corda [\n        \n        ]. However,\nmost permissioned blockchains are based on variants of\nclassical byzantine fault-tolerant (BFT) consensus protocols that\nscale poorly with the number of participants [\n        \n        ].\n      \n        Such scalability limitations stem from bottlenecks both\nat the network and processing levels that result from the\nlarge number of messages that need to be sent, received and\nprocessed to reach consensus. For instance, the well-known\nPBFT protocol [\n        \n        ] organizes participants in a clique and uses\nan all-to-all communication pattern that incurs in a quadratic\nmessage complexity. Although there have been many\nproposals to extend and improve several aspects of PBFT (such\nas [\n        \n        ]), most preserve its communication pattern.\n      \n        In HotStuf [\n        \n        ], only the leader sends/collects messages\ndirectly to/from all other processes, i.e. communication is\nbased on a star topology centered at the leader. This approach\nresults in linear message complexity but the leader is still\nrequired to receive and validate votes from at least 2 + 1\nprocesses. At the time of this writing, the publicly available\nimplementation of HotStuf uses secp256k1 [\n        \n        ], a highly\nefifcient elliptic curve algorithm that is also used in Bitcoin[\n        \n        ].\nIn this implementation, the leader has to relay the full set of\nsignatures to all processes. Alternatively, it is possible to use\nmultisignatures, such as bls [\n        \n        ], to reduce the message size\nat the expense of additional computational load at the leader.\nHowever, due to the centralized control, HotStuf is\ninherently non-scalable: the system performance is limited by the\ncomputing and bandwidth capacity of the leader process.\n      \n        One possible way to circumvent the scalability constraints\nis to select a small committee such as in Algorand [\n        \n        ]\nor SCP [\n        \n        ]. However, this approach either reduces the\nresilience of the system (the maximum number of faults\nbecomes a function of the committee size and not of the entire\nsystem size) or compromises deterministic finality (a block\ncan only be finalized after multiple subsequent blocks have\nbeen produced by diferent committees, implicitly\nvouching for the correctness of the result). Alternatively, systems\nsuch as Steward [\n        \n        ], Fireplug [\n        \n        ], ResilientDB [\n        \n        ] or\nMultiLayer PBFT [\n        \n        ] organize processes in hierarchical groups\nto achieve low message complexity and balance the\nbandwidth load. However, they sacrifice resilience by tolerating\nsignificantly less than  = 3?1 failures.\n      \n        Approaches such as Byzcoin [\n        \n        ], Motor [\n        \n        ], and\nOmniledger [\n        \n        ] address the bottleneck at the leader by\norganizing processes in a tree topology, with the leader placed at\nthe root. The tree is used to disseminate information from\nthe leader to the other processes, and to aggregate votes\nusing cryptographic schemes such as multisignatures [\n        \n        ].\nThe use of trees reduces the number of messages any single\nprocess has to send, receive, and process (that becomes\nlogarithmic with the system size) by distributing the load among\nall internal nodes of the tree.\n      The use of trees comes, however, at the cost of an\nincreased latency of each consensus round. In fact, while in\nPBFT a round can be executed within a single\ncommunication step, in HotStuf it requires two communication steps\n(i.e., a roundtrip), and in trees it requires 2? communication\nsteps, where ? is the height of the tree. If the blockchain\nprotocol only starts a new consensus instance after the previous\none terminates, this increase of the per-round latency has a\ndirect negative impact on the system throughput. In fact, the\nadvantages that stem from the load distribution can easily\nbe outweighed by the disadvantages associated with longer\nconsensus rounds. Strikingly, neither Motor nor Omniledger\ndiscuss or mitigate the impact of the additional latency on\nthe throughput resulting from the increased number of\ncommunication steps required to complete each round.Pipelining allows to mitigate the negative impact on\nthroughput of additional communication steps. In HotStuf the first\nround of the ? consensus instance is executed in parallel\nwith the second round of the (?1)? consensus instance, and\nwith the third round of the ( ? 2)? consensus instance, etc.\nThis allows to piggyback information from multiple\nconsensus instances in a single message. Unfortunately, pipelining\nincreases the burden on the leader further amplifying the\nscalability limitations of a centralized approach.\n        Another disadvantage of trees is their slow recovery in the\npresence of faults. In approaches that use a clique or a star\ntopology, such as PBFT or HotStuf, respectively, the system\nis able to make progress as long as the leader is non-faulty.\nMoreover, if the leader is faulty, the system is guaranteed\nto recover after  + 1 reconfigurations (also known as view\nchanges in the literature). When using trees, progress is\nguaranteed if and only if all internal nodes of the tree are\nnon-faulty (this is a suficient but not necessary condition as\ndiscussed in \u00a73). Furthermore, finding a configuration\nwithout faulty internal nodes has combinatorial complexity [\n        \n        ].\nDue to these challenges, Byzcoin [\n        \n        ] quickly falls back to\na clique when faults occur. Motor [\n        \n        ] and Omniledger [\n        \n        ]\nbuild upon the principles of Byzcoin, but rather than falling\nback immediately to a clique topology, rotate the nodes in\nthe subtrees in an attempt to let the leader, i.e. the root of\nthe tree, gather  ?  signatures. If, in a given round, the\nroot process is unable to collect a quorum of signatures, it\ncontacts directly a random subset of leaf processes, which\nin turn will attempt to collect votes from their siblings,\nuntil a quorum is obtained. Thus, in the worst case scenario,\nassuming a fanout of , after  steps the root will contact\n\nevery other node directly, as if the system was using a star\ntopology. If the root itself fails during this process, a new\ntree is formed but, if more faults occur, the entire procedure\nmay need to be repeated. Furthermore, this strategy only\nworks with trees with a maximum height of ? = 2.\n      Table 1 summarizes our discussion of the systems based\non the criteria discussed above. The table highlights that\nno previous system leverages load balancing techniques to\npromote scalability while preserving high resilience and high\nthroughput. Furthermore, most systems that achieve some\nform of load balancing either decrease fault tolerance or\nincrease the complexity of the reconfiguration leading to a\nslow recovery under faults.In this paper we propose Kauri, a BFT communication\nabstraction that leverages dissemination/aggregation trees\nfor load balancing and scalability while avoiding the main\nlimitations of previous tree-based solutions, namely, poor\nthroughput due to additional round latency and the collapse\nof the tree to a star even in runs with few faults.Kauri introduces novel pipelining techniques suitable for\ntrees of arbitrary depth that sustain high throughput as the\nsystem grows in size. As in HotStuf, Kauri starts a new\ninstance of consensus before the previous instance has\nterminated. But, unlike HotStuf, Kauri starts a new round while\nthe previous round is still being propagated in the tree,\neffectively exploiting the potential parallelism created by the\ndiferent stages (one stage per height) of the tree. This\nallows the leader to efectively use the available bandwidth\nwithout becoming a bottleneck. One of the key challenges\nbehind our combination of trees and pipelining is that\nusing arbitrary pipeline values results in poor performance:\nunder-pipelining fails to take advantage of the available\nparallelization opportunities, while over-pipelining congests the\nsystem - hence, simply using HotSuf?s star-based pipelining\nin Kauri trees would not yield good results. We overcome\nthis challenge by introducing a performance model that\napproximates, for a given scenario, the ideal pipelining values\nthat maximize performance.We also introduce a novel reconfiguration strategy that,\nwhen the number of consecutive faults is small (arguably\nthe most common case), continues to use a tree topology\nrather than falling back to a star. More precisely, for a tree\nfanout of , if  < , Kauri can find a robust tree-based\nconfiguration in  +1 reconfiguration steps (which is optimal),\nand fall back to a star topology only in runs where  ? \nconsecutive faults occur. Thus, Kauri ofers the same\nfaulttolerance of traditional approaches (i.e. ensures correctness\nas long as  <  /3), ensures deterministic finality (unlike\ncommittee-based solutions), and distributes the load among\ninternal nodes allowing it to scale with the number of nodes\nand achieve high throughput. We implemented Kauri and\nevaluated it under diferent realistic scenarios with up to 400\nprocesses. Results show that Kauri outperforms HotStuf?s\nthroughput by up to 28x.In short, the paper makes the following contributions: i)\nWe present a set of abstractions that support the use of\naggregation/dissemination trees in the context of consensus\nprotocols; ii) We introduce a performance model that shows\nhow pipelining can be used to fully leverage the\nparallelisation opportunities ofered by the trees; iii) We present a\nprecise suficient condition that allows eficient\nreconfiguration of the tree without falling back immediately to a star\ntopology; iv) We present Kauri, the first tree-based\ncommunication abstraction for BFT consensus protocols that achieves\nhigher throughput than HotStuf in all considered scenarios\nand better latency under certain conditions; v) We present\nan extensive experimental evaluation of Kauri in realistic\nscenarios with up to 400 nodes.\n2", "System Model": "\n        We assume the system is composed of  server processes\n{1, 2, . . . ,  } and a set of client processes {1, 2, . . . ,  }.\nWe also assume the existence of a Public Key Infrastructure\nused by processes to distribute the keys required for\nauthentication and message signing. Moreover, processes may not\nchange their keys during the execution of the protocol and\nrequire a suficiently lengthy approval process to re-enter\nthe system to avoid rogue key attacks [\n        \n        ]. We assume the\nByzantine fault model, where at most  <  /3 faulty\nprocesses may produce arbitrary values, delay or omit messages,\nand collude with each other, but do not possess suficient\nresources to compromise the cryptographic primitives.\n      \n        Processes communicate via perfect point-to-point\nchannels with the following properties: Validity: If a process  \ndelivers a value  on a channel over an edge   ,  was sent\nby  . Termination: If both  and   are correct, if  invokes\nsend then eventually   delivers  . These are implemented\nusing mechanisms for message re-transmission and detection\nand suppression of duplicates [\n        \n        ]. To circumvent the\nimpossibility of consensus[\n        \n        ], we assume the partial synchrony\nmodel [\n        \n        ]. In this model, there may be an unstable period,\nwhere messages exchanged between correct processes are\narbitrarily delayed. However, there is a known bound ? on\nthe worst-case network latency and an unknown Global\nStabilization Time (GST), such that after GST, all messages\nbetween correct processes arrive within ?. Note that safety\nis always preserved and the partial synchrony assumptions\nare necessary only to ensure liveness [\n        \n        ].\n3\n      ", "Using Dissemination/Aggregation Trees": "Instead of designing a completely new consensus algorithm\nfrom scratch, we developed Kauri as an extension of HotStuf.\nThe key idea is to replace the dissemination and aggregation\npatterns used by HotStuf, which are based on a star\ntopology, by new patterns based on tree topologies. While for\nsimplicity our presentation hinges on HotStuf\ncharacteristics, our principles could also be applied to other leader-based\nconsensus algorithms.For self-containment, we provide a brief high-level\ndescription of HotStuf. We give emphasis on the communication\npattern used in HotStuf and discuss how this pattern may be\nabstracted, such that it can be replaced by diferent\nimplementations. HotStuf reaches consensus in four communication\nrounds. Each round consists of two phases: i) a dissemination\nphase where the leader broadcasts some information to all\nprocesses; and ii) an aggregation phase where the leader\ncollects and aggregates information from a quorum of replicas.\nAll rounds follow the same exact pattern, but the information\nsent and received by the leader in each round difers:\nFirst round: In the dissemination phase, the leader broadcasts\na block proposal to all processes. In the aggregation phase,\nthe leader collects a prepare quorum of  ?  signatures\nof the block. The signatures convey that the replicas have\nvalidated and accepted the block proposed by the leader.\nSecond round: In the dissemination phase, the leader\nbroadcasts the prepare quorum. In the aggregation phase, the leader\ncollects the pre-commit quorum, including  ?  signatures\nfrom processes that have validated the prepare quorum. If\nthe leader is able to collect a pre-commit quorum, the value\nproposed by the leader is locked and will not be changed,\neven if the leader is subsequently suspected.Third round: In the dissemination phase, the leader broadcasts\nthe pre-commit quorum. In the aggregation phase, the leader\ncollects a commit quorum, including  ?  signatures of\nprocesses that have validated the pre-commit quorum. If the\nleader is able to collect the quorum, the value is decided.\n1: prepare 2: pre-commit 3: commit 4: decide\n1: prepare\n2: pre-commit\n3: commit\n4: decideP0\nP1P2\nP3P4 P5\n(a) TopologyC\nP0\nP1\nP2\nL1\nL2L3\nP6 L4\nP1\nP5P2P4\nP6P0P3\nFourth round: In the last round, the leader broadcasts the\ncommit quorum to all processes, which in turn verify it and\ndecide accordingly.\n? broadcastMsg(data). This primitive is used in the first\nphase of each round to broadcast data from the leader\nto all other processes.\n? waitFor (N - f) votes. This primitive is used in the second\nphase of each round, for the leader to collect votes from\na quorum of  ?  processes.The implementation of these primitives must satisfy the\nfollowing properties:\nDefinition 1. Reliable Dissemination: After the GST, and\nin a robust configuration, all correct processes deliver the data\nsent by the leader.Definition 2. Fulfillment: After the GST, and in a robust\nconfiguration, the aggregate collected by the leader includes at\nleast  ?  votes.It is easy to show that, when using perfect point-to-point\nchannels, it is possible to achieve Reliable Dissemination\nand Fulfillment on a star topology. For this purpose, we first\ndefine the notion of a robust star.Definition 3. Robust Star: A star is said to be robust if the\nleader is correct, and non-robust if the leader is faulty.\n          Briefly, the assumption of perfect point-to-point channels\nand the fact that the leader is correct ensure that all correct\nprocesses deliver the message sent by the leader, hence\nsatisfying Reliable Dissemination. In a similar fashion, all correct\nprocesses are able to send their vote to the leader which,\nin turn, is able to collect  ?  votes/signatures and hence\nsatisfy Fulfillment. For lack of space, we refer the reader to\nthe HotStuf paper for full details [\n          \n          ].\n3.2\n        We now discuss how to implement the broadcastMsg and\nwaitFor primitives using tree topologies, while preserving\nthe same properties. As noted before, processes are organized\nin a tree with the leader at the root. The primitive\nbroadcastMsg is implemented by having the root send data to its\nchildren that in turn forward it to their own children, and so\nforth. The primitive waitFor is implemented by having the\nleaf nodes send their signatures to their parent. The parent\nthen aggregates those signatures with its own and sends the\naggregate to their own parent. This process is repeated until\nthe final aggregate is computed at the root of the tree. This\nprocess is illustrated in Figure 2.When using a tree to implement broadcastMsg and waitFor,\nthe notion of robust configuration needs to be adapted, as\nit is no longer enough that the leader is non-faulty to make\nthe configuration robust. We define a robust tree as follows.\nDefinition 4. Robust Tree: An edge is said to be safe if the\ncorresponding vertices are both correct processes. A tree is robust\nif the leader process is correct and, for every pair of correct\nprocess  and   , the path in the tree connecting these processes\nis composed exclusively of safe edges.Note that our definition of a robust tree is a suficient but\nnot necessary condition to achieve consensus. In fact,\nconsensus can be reached as long as there is a path composed\nexclusively of safe edges between the leader and a quorum\nof correct processes. Our simpler formulation discards some\nviable configurations - for instance a tree with a faulty\ninternal node where all its children are also faulty - but provides\nan important corollary: a tree is robust if and only if all\ninternal nodes, including the leader, are correct processes. This\nobservation allows us to devise an eficient reconfiguration\nalgorithm that is optimal when the number of consecutive\nfaults is small (\u00a75).We start by describing the communication primitives used\nto propagate information on the tree and the cryptographic\nprimitives used to perform aggregation.\n3.3.1 Communicating on the Tree. Processes use the\ntree to communicate. Each directed edge maps to a perfect\nsingle-use point-to-point channel used to send and deliver\na single value. Note that, when using perfect channels, a\nmessage is only guaranteed to be eventually delivered if\nboth the sender and the recipient are correct. If the sender is\nfaulty, no message may ever be delivered. To avoid blocking,\na process should be able to make progress if a message takes\ntoo long to be received. Moreover, the single-use ensures\nthat the receiver either returns the current value sent or ?\nAlgorithm 1 Impatient Channels: receive\n1: let ic be an impatient channel built on top of perfect channel pc\n2: function ic.receive( p)\n3: timer.start(?)\n4: when pc.deliver (, ) do return \n5: when timer.timeout() do return ?? receive from p\nand never older values. In practice, this can be achieved by\nassigning a unique identifier to each instance and tagging\nthe corresponding messages with this identifier.This behavior is captured by an abstraction we call\nimpatient channels. Impatient channels ofer a blocking receive\nprimitive that always returns a value: either the value sent\nby the sender, or a special value ? if the sender is faulty or\nthe system is unstable. After the GST, if the sender and the\nreceiver are correct, the receiver always receives the value\nsent. Impatient channels have the following properties:\n? Validity: If a process   delivers a value  on a channel\nover an edge   ,  was sent by  or  = ?.\n? Termination: If a correct process   invokes receive,\nit eventually returns some value.\n? Conditional Accuracy: Let  and   be correct sender\nand receiver processes, respectively. After GST,  \nalways return the value  sent by  .Algorithm 1 shows how impatient channels can be\nimplemented on top of perfect channels using the known bound\n? on the worst-case network latency.\n          sensus, it is necessary to collect a Byzantine quorum of votes.\nThe collection and validation of these votes can be an\nimpairment for scalability. Kauri mitigates these costs by using the\ntree to aggregate votes as they are forwarded to the leader.\nWe model the process of vote aggregation with a\ncryptographic collection abstraction that corresponds to a secure\nmulti-set of tuples (,  ). A process  can create a new\ncollection  with a value  by calling =new((,  )). Processes\ncan also merge two collections using a combine primitive\ndenoted by 12 = 1 ? 2. A process can also check if a collection\n includes at least a given threshold of  distinct tuples with\nthe same value  , by calling has(, ,  ). Finally, it is possible\nto check the total number of input tuples combined in  by\nchecking its cardinality | |. Cryptographic collections have\nthe following properties:\n? Commutativity: 1 ? 2 = 2 ? 1\n? Associativity: 1 ? (2 ? 3) = (1 ? 2) ? 3\n? Idempotency: 1 ? 1 = 1\n? Integrity: Let  = 1 ? . . .  . . . . If has (, ,  ) then at\nleast  distinct processes  have executed  =new((,  ))\nNote that diferent cryptographic techniques can be used\nto implement these collections. In Kauri, we leverage a\nnoninteractive bls multisignature scheme that allows each\ninternal node to aggregate the votes from its children into\none single aggregated vote [\n          \n          ]. The burden imposed on each\ninternal node (including the root) is O (), where  is the\n        \n          Algorithm 2 broadcastMsg on a tree  (process  )\n1: procedure broadcastMsg( , data)\n2: children ?  .children( )\n3: parent ?  .parent( )\n4: if parent ? ? then\n5: data ? ic.receive(parent)\n6: end if\n7: for all e ? children do\n8: ic.send(, data)\n9: end for\n10: return data\n11: end procedure\nfanout of the tree and the complexity of verifying an\naggregated vote is O (1). Note that classical asymmetric signatures\nrequire O ( ) verifications at each process [\n          \n          ].\n3.3.3 Implementing broadcastMsg. The implementation\nof broadcastMsg on a tree is presented in Algorithm 2. Note\nthat the algorithm always terminates, even if some\nintermediate nodes are faulty. This is guaranteed since impatient\nchannels always return a value after the known bound ? on\nthe worst-case network latency, either the data sent by the\nparent or the special value ?.\n        Theorem 1. Algorithm 2 guarantees Reliable Dissemination.\nProof. We prove this by contradiction. Assume Reliable\nDissemination is not guaranteed. This implies that at least one\ncorrect process did not receive the data sent by the leader.\nThis is only possible if: i) at least one correct process is not\nconnected to the leader either directly or through correct\nintermediary processes, ii) one of the intermediary processes\nor the root process did not invoke channel.send for at least\none correct child process, or iii) the data got lost in the\nchannel. Reliable Dissemination is defined only for a robust\nconifguration which, following the definition of a Robust Tree,\nensures that the leader is correct and there is a path of correct\nprocesses between the leader and any other correct process.\nThus, the first case is not possible. Moreover, correct\nprocesses follow the algorithm and, because correct processes\ncan only have correct parents in a robust configuration, the\nsecond case is also impossible. Finally, the third case is also\nimpossible due to the use of perfect channels. Therefore,\nAlgorithm 2 guarantees Reliable Dissemination. ?\n3.3.4 Implementing waitFor. Algorithm 3 presents the\nimplementation of waitFor on a tree. The algorithm relies\non the cryptographic primitives to aggregate the signatures\nas they are propagated toward the root. Like broadcastMsg,\nwaitFor always terminates, even if some nodes are faulty.\nThis is guaranteed because impatient channels always return\na value after the known bound ? on the worst-case network\nlatency, either the data sent by the child processes or the\nspecial value ?. Before GST or in non-robust configurations,\nthe collection returned at the leader may be empty or include\njust a subset of the required signatures.Theorem 2. Algorithm 3 guarantees Fulfillment.\nAlgorithm 3 waitFor on a tree  (process  )\n1: procedure waitFor( , input)\n2: children ?  .children( )\n3: parent ?  .parent( )\n4: collection ? new( (, input))\n5: for all e ? children do\n6: partial ? ic.receive()\n7: collection ? collection ? partial\n8: end for\n9: if parent ? ? then\n10: ic.send(parent, collection)\n11: end if\n12: return collection\n13: end procedure\nProof. We prove this by contradiction. Assume that the leader\nprocess was unable to collect  ?  signatures. Following\nAlgorithm 3, this means that either: i) an internal node did not\nreceive the signatures from all correct children (line 6), ii) or\nan internal node did not aggregate and relay the signatures\nit has received from its correct children (line 10). Since we\nassume impatient channels, that are implemented on top of\nperfect point-to-point channels, the first case is not possible\nafter GST. The second case may happen, if either the internal\nnode omits signatures in the aggregate, does not relay any\nsignatures, or is blocked waiting indefinitely for messages\nfrom its children. Either option leads to a contradiction. Since\nwe assume a robust graph, all internal nodes between the\nroot and a correct process must be correct and hence follow\nthe algorithm. Additionally, due to the impatient channels,\neventually each channel will return a value to the internal\nnode making sure that eventually it will unblock and\nrelay all collected signatures from all correct child processes.\nTherefore Algorithm 3 guarantees Fulfillment. ?of the broadcastMsg and waitFor primitives that we\nintroduced above allow us to replace the star topology used in\nHotStuf with a tree topology that is more eficient and scalable\nas we show in the evaluation (\u00a77). However, two remaining\nchallenges need to be addressed to make the tree topology a\nvalid alternative in practice:\nMitigate the increased latency: While trees allow to distribute\nthe load among all processes, the additional round-trip of\nthe broadcastMsg and waitFor primitives result in additional\nlatency, which might negatively afect system throughput.\nWe discuss how to mitigate this in \u00a74.Reconfiguration strategy: In HotStuf, the configuration is\nrobust if the leader is non-faulty. Therefore, there are only \nnon-robust configurations and  ?  robust configurations. It\nis thus trivial to devise a reconfiguration strategy that yields a\nrobust configuration in an optimal number of steps (i.e.  + 1).\nIn a tree, a configuration is robust if the root and all internal\nnodes are correct. The total number of configurations and\nthe subset of non-robust configurations is extremely large. In\n\u00a75 we introduce a reconfiguration strategy that builds robust\nconfigurations in a small number of steps.\n4", "3.1 HotStuf Communication Pattern": "For self-containment, we provide a brief high-level\ndescription of HotStuf. We give emphasis on the communication\npattern used in HotStuf and discuss how this pattern may be\nabstracted, such that it can be replaced by diferent\nimplementations. HotStuf reaches consensus in four communication\nrounds. Each round consists of two phases: i) a dissemination\nphase where the leader broadcasts some information to all\nprocesses; and ii) an aggregation phase where the leader\ncollects and aggregates information from a quorum of replicas.\nAll rounds follow the same exact pattern, but the information\nsent and received by the leader in each round difers:\nFirst round: In the dissemination phase, the leader broadcasts\na block proposal to all processes. In the aggregation phase,\nthe leader collects a prepare quorum of  ?  signatures\nof the block. The signatures convey that the replicas have\nvalidated and accepted the block proposed by the leader.\nSecond round: In the dissemination phase, the leader\nbroadcasts the prepare quorum. In the aggregation phase, the leader\ncollects the pre-commit quorum, including  ?  signatures\nfrom processes that have validated the prepare quorum. If\nthe leader is able to collect a pre-commit quorum, the value\nproposed by the leader is locked and will not be changed,\neven if the leader is subsequently suspected.Third round: In the dissemination phase, the leader broadcasts\nthe pre-commit quorum. In the aggregation phase, the leader\ncollects a commit quorum, including  ?  signatures of\nprocesses that have validated the pre-commit quorum. If the\nleader is able to collect the quorum, the value is decided.\n1: prepare 2: pre-commit 3: commit 4: decide\n1: prepare\n2: pre-commit\n3: commit\n4: decideP0\nP1P2\nP3P4 P5\n(a) TopologyC\nP0\nP1\nP2\nL1\nL2L3\nP6 L4\nP1\nP5P2P4\nP6P0P3\nFourth round: In the last round, the leader broadcasts the\ncommit quorum to all processes, which in turn verify it and\ndecide accordingly.\n? broadcastMsg(data). This primitive is used in the first\nphase of each round to broadcast data from the leader\nto all other processes.\n? waitFor (N - f) votes. This primitive is used in the second\nphase of each round, for the leader to collect votes from\na quorum of  ?  processes.The implementation of these primitives must satisfy the\nfollowing properties:\nDefinition 1. Reliable Dissemination: After the GST, and\nin a robust configuration, all correct processes deliver the data\nsent by the leader.Definition 2. Fulfillment: After the GST, and in a robust\nconfiguration, the aggregate collected by the leader includes at\nleast  ?  votes.It is easy to show that, when using perfect point-to-point\nchannels, it is possible to achieve Reliable Dissemination\nand Fulfillment on a star topology. For this purpose, we first\ndefine the notion of a robust star.Definition 3. Robust Star: A star is said to be robust if the\nleader is correct, and non-robust if the leader is faulty.\n          Briefly, the assumption of perfect point-to-point channels\nand the fact that the leader is correct ensure that all correct\nprocesses deliver the message sent by the leader, hence\nsatisfying Reliable Dissemination. In a similar fashion, all correct\nprocesses are able to send their vote to the leader which,\nin turn, is able to collect  ?  votes/signatures and hence\nsatisfy Fulfillment. For lack of space, we refer the reader to\nthe HotStuf paper for full details [\n          \n          ].\n3.2\n        ", "Using Trees to Implement HotStuf": "We now discuss how to implement the broadcastMsg and\nwaitFor primitives using tree topologies, while preserving\nthe same properties. As noted before, processes are organized\nin a tree with the leader at the root. The primitive\nbroadcastMsg is implemented by having the root send data to its\nchildren that in turn forward it to their own children, and so\nforth. The primitive waitFor is implemented by having the\nleaf nodes send their signatures to their parent. The parent\nthen aggregates those signatures with its own and sends the\naggregate to their own parent. This process is repeated until\nthe final aggregate is computed at the root of the tree. This\nprocess is illustrated in Figure 2.When using a tree to implement broadcastMsg and waitFor,\nthe notion of robust configuration needs to be adapted, as\nit is no longer enough that the leader is non-faulty to make\nthe configuration robust. We define a robust tree as follows.\nDefinition 4. Robust Tree: An edge is said to be safe if the\ncorresponding vertices are both correct processes. A tree is robust\nif the leader process is correct and, for every pair of correct\nprocess  and   , the path in the tree connecting these processes\nis composed exclusively of safe edges.Note that our definition of a robust tree is a suficient but\nnot necessary condition to achieve consensus. In fact,\nconsensus can be reached as long as there is a path composed\nexclusively of safe edges between the leader and a quorum\nof correct processes. Our simpler formulation discards some\nviable configurations - for instance a tree with a faulty\ninternal node where all its children are also faulty - but provides\nan important corollary: a tree is robust if and only if all\ninternal nodes, including the leader, are correct processes. This\nobservation allows us to devise an eficient reconfiguration\nalgorithm that is optimal when the number of consecutive\nfaults is small (\u00a75).", "3.3 Dissemination and Aggregation": "We start by describing the communication primitives used\nto propagate information on the tree and the cryptographic\nprimitives used to perform aggregation.\n3.3.1 Communicating on the Tree. Processes use the\ntree to communicate. Each directed edge maps to a perfect\nsingle-use point-to-point channel used to send and deliver\na single value. Note that, when using perfect channels, a\nmessage is only guaranteed to be eventually delivered if\nboth the sender and the recipient are correct. If the sender is\nfaulty, no message may ever be delivered. To avoid blocking,\na process should be able to make progress if a message takes\ntoo long to be received. Moreover, the single-use ensures\nthat the receiver either returns the current value sent or ?\nAlgorithm 1 Impatient Channels: receive\n1: let ic be an impatient channel built on top of perfect channel pc\n2: function ic.receive( p)\n3: timer.start(?)\n4: when pc.deliver (, ) do return \n5: when timer.timeout() do return ?? receive from p\nand never older values. In practice, this can be achieved by\nassigning a unique identifier to each instance and tagging\nthe corresponding messages with this identifier.This behavior is captured by an abstraction we call\nimpatient channels. Impatient channels ofer a blocking receive\nprimitive that always returns a value: either the value sent\nby the sender, or a special value ? if the sender is faulty or\nthe system is unstable. After the GST, if the sender and the\nreceiver are correct, the receiver always receives the value\nsent. Impatient channels have the following properties:\n? Validity: If a process   delivers a value  on a channel\nover an edge   ,  was sent by  or  = ?.\n? Termination: If a correct process   invokes receive,\nit eventually returns some value.\n? Conditional Accuracy: Let  and   be correct sender\nand receiver processes, respectively. After GST,  \nalways return the value  sent by  .Algorithm 1 shows how impatient channels can be\nimplemented on top of perfect channels using the known bound\n? on the worst-case network latency.", "6: end function": "? receive from p\nand never older values. In practice, this can be achieved by\nassigning a unique identifier to each instance and tagging\nthe corresponding messages with this identifier.This behavior is captured by an abstraction we call\nimpatient channels. Impatient channels ofer a blocking receive\nprimitive that always returns a value: either the value sent\nby the sender, or a special value ? if the sender is faulty or\nthe system is unstable. After the GST, if the sender and the\nreceiver are correct, the receiver always receives the value\nsent. Impatient channels have the following properties:\n? Validity: If a process   delivers a value  on a channel\nover an edge   ,  was sent by  or  = ?.\n? Termination: If a correct process   invokes receive,\nit eventually returns some value.\n? Conditional Accuracy: Let  and   be correct sender\nand receiver processes, respectively. After GST,  \nalways return the value  sent by  .Algorithm 1 shows how impatient channels can be\nimplemented on top of perfect channels using the known bound\n? on the worst-case network latency.", "3.3.2 Cryptographic Collections. In each round of con": "\n          sensus, it is necessary to collect a Byzantine quorum of votes.\nThe collection and validation of these votes can be an\nimpairment for scalability. Kauri mitigates these costs by using the\ntree to aggregate votes as they are forwarded to the leader.\nWe model the process of vote aggregation with a\ncryptographic collection abstraction that corresponds to a secure\nmulti-set of tuples (,  ). A process  can create a new\ncollection  with a value  by calling =new((,  )). Processes\ncan also merge two collections using a combine primitive\ndenoted by 12 = 1 ? 2. A process can also check if a collection\n includes at least a given threshold of  distinct tuples with\nthe same value  , by calling has(, ,  ). Finally, it is possible\nto check the total number of input tuples combined in  by\nchecking its cardinality | |. Cryptographic collections have\nthe following properties:\n? Commutativity: 1 ? 2 = 2 ? 1\n? Associativity: 1 ? (2 ? 3) = (1 ? 2) ? 3\n? Idempotency: 1 ? 1 = 1\n? Integrity: Let  = 1 ? . . .  . . . . If has (, ,  ) then at\nleast  distinct processes  have executed  =new((,  ))\nNote that diferent cryptographic techniques can be used\nto implement these collections. In Kauri, we leverage a\nnoninteractive bls multisignature scheme that allows each\ninternal node to aggregate the votes from its children into\none single aggregated vote [\n          \n          ]. The burden imposed on each\ninternal node (including the root) is O (), where  is the\n        \n          Algorithm 2 broadcastMsg on a tree  (process  )\n1: procedure broadcastMsg( , data)\n2: children ?  .children( )\n3: parent ?  .parent( )\n4: if parent ? ? then\n5: data ? ic.receive(parent)\n6: end if\n7: for all e ? children do\n8: ic.send(, data)\n9: end for\n10: return data\n11: end procedure\nfanout of the tree and the complexity of verifying an\naggregated vote is O (1). Note that classical asymmetric signatures\nrequire O ( ) verifications at each process [\n          \n          ].\n3.3.3 Implementing broadcastMsg. The implementation\nof broadcastMsg on a tree is presented in Algorithm 2. Note\nthat the algorithm always terminates, even if some\nintermediate nodes are faulty. This is guaranteed since impatient\nchannels always return a value after the known bound ? on\nthe worst-case network latency, either the data sent by the\nparent or the special value ?.\n        Theorem 1. Algorithm 2 guarantees Reliable Dissemination.\nProof. We prove this by contradiction. Assume Reliable\nDissemination is not guaranteed. This implies that at least one\ncorrect process did not receive the data sent by the leader.\nThis is only possible if: i) at least one correct process is not\nconnected to the leader either directly or through correct\nintermediary processes, ii) one of the intermediary processes\nor the root process did not invoke channel.send for at least\none correct child process, or iii) the data got lost in the\nchannel. Reliable Dissemination is defined only for a robust\nconifguration which, following the definition of a Robust Tree,\nensures that the leader is correct and there is a path of correct\nprocesses between the leader and any other correct process.\nThus, the first case is not possible. Moreover, correct\nprocesses follow the algorithm and, because correct processes\ncan only have correct parents in a robust configuration, the\nsecond case is also impossible. Finally, the third case is also\nimpossible due to the use of perfect channels. Therefore,\nAlgorithm 2 guarantees Reliable Dissemination. ?\n3.3.4 Implementing waitFor. Algorithm 3 presents the\nimplementation of waitFor on a tree. The algorithm relies\non the cryptographic primitives to aggregate the signatures\nas they are propagated toward the root. Like broadcastMsg,\nwaitFor always terminates, even if some nodes are faulty.\nThis is guaranteed because impatient channels always return\na value after the known bound ? on the worst-case network\nlatency, either the data sent by the child processes or the\nspecial value ?. Before GST or in non-robust configurations,\nthe collection returned at the leader may be empty or include\njust a subset of the required signatures.Theorem 2. Algorithm 3 guarantees Fulfillment.\nAlgorithm 3 waitFor on a tree  (process  )\n1: procedure waitFor( , input)\n2: children ?  .children( )\n3: parent ?  .parent( )\n4: collection ? new( (, input))\n5: for all e ? children do\n6: partial ? ic.receive()\n7: collection ? collection ? partial\n8: end for\n9: if parent ? ? then\n10: ic.send(parent, collection)\n11: end if\n12: return collection\n13: end procedure\nProof. We prove this by contradiction. Assume that the leader\nprocess was unable to collect  ?  signatures. Following\nAlgorithm 3, this means that either: i) an internal node did not\nreceive the signatures from all correct children (line 6), ii) or\nan internal node did not aggregate and relay the signatures\nit has received from its correct children (line 10). Since we\nassume impatient channels, that are implemented on top of\nperfect point-to-point channels, the first case is not possible\nafter GST. The second case may happen, if either the internal\nnode omits signatures in the aggregate, does not relay any\nsignatures, or is blocked waiting indefinitely for messages\nfrom its children. Either option leads to a contradiction. Since\nwe assume a robust graph, all internal nodes between the\nroot and a correct process must be correct and hence follow\nthe algorithm. Additionally, due to the impatient channels,\neventually each channel will return a value to the internal\nnode making sure that eventually it will unblock and\nrelay all collected signatures from all correct child processes.\nTherefore Algorithm 3 guarantees Fulfillment. ?", "3.3.5 Challenges of Using a Tree. The implementations": "of the broadcastMsg and waitFor primitives that we\nintroduced above allow us to replace the star topology used in\nHotStuf with a tree topology that is more eficient and scalable\nas we show in the evaluation (\u00a77). However, two remaining\nchallenges need to be addressed to make the tree topology a\nvalid alternative in practice:\nMitigate the increased latency: While trees allow to distribute\nthe load among all processes, the additional round-trip of\nthe broadcastMsg and waitFor primitives result in additional\nlatency, which might negatively afect system throughput.\nWe discuss how to mitigate this in \u00a74.Reconfiguration strategy: In HotStuf, the configuration is\nrobust if the leader is non-faulty. Therefore, there are only \nnon-robust configurations and  ?  robust configurations. It\nis thus trivial to devise a reconfiguration strategy that yields a\nrobust configuration in an optimal number of steps (i.e.  + 1).\nIn a tree, a configuration is robust if the root and all internal\nnodes are correct. The total number of configurations and\nthe subset of non-robust configurations is extremely large. In\n\u00a75 we introduce a reconfiguration strategy that builds robust\nconfigurations in a small number of steps.\n4", "Mitigating Tree Latency": "In this section we introduce the mechanisms to mitigate the\nadditional latency inherent to tree topologies when compared\nto HotStuf?s star topology. As described earlier, HotStuf\nneeds four communication rounds for each instance of\nconsensus. If HotStuf waited for each consensus instance to\nterminate before starting the next one, the system\nthroughput would sufer significantly. Therefore, HotStuf relies on\na pipelining optimization, where the  + 1 instance of\nconsensus is started optimistically, before instance  is terminated.\nAs a result, at any given time, each process participates in\nmultiple consensus instances. Furthermore, to reduce the\nnumber of messages, HotStuf combines the information of\nthese parallel consensus instances in a single message.By following the same structure, Kauri is amenable to the\nsame optimization. However, because Kauri uses a tree, the\nlatency to terminate a given round (and hence a consensus\ninstance) is substantially larger than in HotStuf. While at\nifrst this might look like an obstacle, it opens the door for\nmore advanced pipelining techniques that substantially\nimprove throughput and hide the additional latency induced by\ntrees. In fact, as we will show in the evaluation (\u00a77), in certain\nscenarios, Kauri can achieve not only higher throughput, but\nalso lower latency than HotStuf.\n4.1We start by providing an overview of HotStuf?s pipelining\nusing the seven node scenario previously introduced in\nFigure 1. Figure 3 illustrates the execution of multiple rounds\nof consensus in HotStuf where each round is depicted in a\ndiferent shade of gray.Consider the first round (light gray) that starts with the\nleader sending the block to all other processes (downward\narrows). The time this step takes depends on the size of the\ndata being transmitted, the available bandwidth, and the\ntotal number of nodes. To conclude a round, the leader has to\ncollect a quorum of signatures. These signatures start\nflowing toward the leader as soon as the first process receives\nthe message from the leader (upward arrows). Therefore,\nin a given round, dissemination and aggregation are\npartially executed in parallel. Soon after the dissemination of a\nround finishes, the leader may already start the next round\nof consensus.To implement pipelining, HotStuf optimistically starts a\nnew instance of consensus by piggybacking the first round\nmessages of the next consensus instance with the second\nround messages of the previous instance. Because HotStuf\nrequires four rounds of communication, this process can be\nrepeated multiple times, resulting in messages that carry\ninformation of up to four pipelined consensus instances. In\nHotStuf the pipelining depth (i.e. the maximum number of\nconsensus instances that can run in parallel) is thus equal to\nthe number of communication rounds.\nSending time Remaining timeComputation\u00a0time\nInstance 1,Round 1IInnssttaannccee 21+,, RRoouunndd 12IIInnnssstttaaannnccceee 213++,,, RRRooouuunnnddd 231In Kauri, we extend pipelining to fully leverage the load\nbalancing properties of trees as illustrated in Figure 4. In a tree,\nthe fanout  is much smaller than the number of nodes  ,\nand therefore in Kauri the root completes its dissemination\nphase much faster than in HotStuf, and it may become idle\nlong before it starts collecting votes. This allows the root to\nstart multiple instances of consensus during the execution\nof a single consensus round. This introduces a multiplicative\nfactor that we call the pipelining stretch that augments the\npipelining depth of HotStuf. In the example of Figure 4, the\nleader is able to start 3 new instances during the execution\nof the first round of a given consensus instance. Note that,\nin this example, the messages from the second round of\ninstance 1 are piggybacked with messages from the first round\nof instance 4, i.e. a message carries information from\nconsensus instances/rounds that are farther away in the pipeline.\nThis increase in the pipelining depth allows for a higher\ndegree of parallelism, and hence throughput.Kauri?s pipelining stretch, i.e. the number of instances that\ncan be initiated during a single round, is afected by the\nfollowing parameters:\nSending time: the time a node takes to send a block to all its\nchildren. This value is a function of the fanout , the block\nsize , and the link bandwidth , and is approximated by  .Processing time: the time a node takes to validate and/or\naggregate the votes it receives from its children. This heavily\ndepends on the type of signatures used by the algorithm. We\nmeasured these values experimentally, for diferent signature\nschemes (see \u00a77).Remaining time: the time that elapses from point the root\nifnishes sending the block to its children until it receives and\nprocesses the last reply. This value is a sum of the network\nlatency and the processing time as defined above. It is roughly\ngiven by:remaining time = ? \u00b7 (RTT + processing time)\nwhere ? is the height of the tree and RTT is the network\nroundtrip time. In a star topology the remaining time is small\nand mainly used to collect and process replies. However, in\na tree, the root is often idle for most of the remaining time.Kauri leverages this larger remaining time to start\nadditional consensus instances. The challenge is therefore to\nestimate how many additional instances can be started, i.e.\nto estimate the pipelining stretch. For presentation simplicity,\nwe assume that sending and processing can be performed\nconcurrently. In a system where the bottleneck is the\nbandwidth, i.e. where the sending time is much larger than the\nprocessing time, the number of additional consensus instance\nthat can be started during the remaining time is given by:\nremaining time . Similarly, in a system where the bottleneck is the\nsending time\nCPU, i.e. where the processing time is much larger than the\nsending time, the number of additional consensus instances\nthat can be started during the remaining time is given by:\nremaining time .\nprocessing timeKauri?s pipelining stretch allows us to make the best use\nof the time the leader saves by having to interact with just \nnodes instead of  ? 1 nodes. Therefore, the ratio between\n ? 1 and  defines the maximum speedup we can achieve\nby using a tree instead of a star. For instance, in a system of\n400 nodes, organized in a tree with fanout 20, the maximum\nspeedup we can expect Kauri to ofer is 19.95.\n5", "Pipelining in HotStuf": "We start by providing an overview of HotStuf?s pipelining\nusing the seven node scenario previously introduced in\nFigure 1. Figure 3 illustrates the execution of multiple rounds\nof consensus in HotStuf where each round is depicted in a\ndiferent shade of gray.Consider the first round (light gray) that starts with the\nleader sending the block to all other processes (downward\narrows). The time this step takes depends on the size of the\ndata being transmitted, the available bandwidth, and the\ntotal number of nodes. To conclude a round, the leader has to\ncollect a quorum of signatures. These signatures start\nflowing toward the leader as soon as the first process receives\nthe message from the leader (upward arrows). Therefore,\nin a given round, dissemination and aggregation are\npartially executed in parallel. Soon after the dissemination of a\nround finishes, the leader may already start the next round\nof consensus.To implement pipelining, HotStuf optimistically starts a\nnew instance of consensus by piggybacking the first round\nmessages of the next consensus instance with the second\nround messages of the previous instance. Because HotStuf\nrequires four rounds of communication, this process can be\nrepeated multiple times, resulting in messages that carry\ninformation of up to four pipelined consensus instances. In\nHotStuf the pipelining depth (i.e. the maximum number of\nconsensus instances that can run in parallel) is thus equal to\nthe number of communication rounds.\nSending time Remaining timeComputation\u00a0time\nInstance 1,Round 1IInnssttaannccee 21+,, RRoouunndd 12IIInnnssstttaaannnccceee 213++,,, RRRooouuunnnddd 231", "4.2 Pipelining in Kauri": "In Kauri, we extend pipelining to fully leverage the load\nbalancing properties of trees as illustrated in Figure 4. In a tree,\nthe fanout  is much smaller than the number of nodes  ,\nand therefore in Kauri the root completes its dissemination\nphase much faster than in HotStuf, and it may become idle\nlong before it starts collecting votes. This allows the root to\nstart multiple instances of consensus during the execution\nof a single consensus round. This introduces a multiplicative\nfactor that we call the pipelining stretch that augments the\npipelining depth of HotStuf. In the example of Figure 4, the\nleader is able to start 3 new instances during the execution\nof the first round of a given consensus instance. Note that,\nin this example, the messages from the second round of\ninstance 1 are piggybacked with messages from the first round\nof instance 4, i.e. a message carries information from\nconsensus instances/rounds that are farther away in the pipeline.\nThis increase in the pipelining depth allows for a higher\ndegree of parallelism, and hence throughput.", "4.3 Pipelining Stretch and Expected Speedup": "Kauri?s pipelining stretch, i.e. the number of instances that\ncan be initiated during a single round, is afected by the\nfollowing parameters:\nSending time: the time a node takes to send a block to all its\nchildren. This value is a function of the fanout , the block\nsize , and the link bandwidth , and is approximated by  .Processing time: the time a node takes to validate and/or\naggregate the votes it receives from its children. This heavily\ndepends on the type of signatures used by the algorithm. We\nmeasured these values experimentally, for diferent signature\nschemes (see \u00a77).Remaining time: the time that elapses from point the root\nifnishes sending the block to its children until it receives and\nprocesses the last reply. This value is a sum of the network\nlatency and the processing time as defined above. It is roughly\ngiven by:remaining time = ? \u00b7 (RTT + processing time)\nwhere ? is the height of the tree and RTT is the network\nroundtrip time. In a star topology the remaining time is small\nand mainly used to collect and process replies. However, in\na tree, the root is often idle for most of the remaining time.Kauri leverages this larger remaining time to start\nadditional consensus instances. The challenge is therefore to\nestimate how many additional instances can be started, i.e.\nto estimate the pipelining stretch. For presentation simplicity,\nwe assume that sending and processing can be performed\nconcurrently. In a system where the bottleneck is the\nbandwidth, i.e. where the sending time is much larger than the\nprocessing time, the number of additional consensus instance\nthat can be started during the remaining time is given by:\nremaining time . Similarly, in a system where the bottleneck is the\nsending time\nCPU, i.e. where the processing time is much larger than the\nsending time, the number of additional consensus instances\nthat can be started during the remaining time is given by:\nremaining time .\nprocessing timeKauri?s pipelining stretch allows us to make the best use\nof the time the leader saves by having to interact with just \nnodes instead of  ? 1 nodes. Therefore, the ratio between\n ? 1 and  defines the maximum speedup we can achieve\nby using a tree instead of a star. For instance, in a system of\n400 nodes, organized in a tree with fanout 20, the maximum\nspeedup we can expect Kauri to ofer is 19.95.\n5", "Reconfiguration": "We now discuss Kauri?s reconfiguration strategy. Recall that,\nin Kauri, processes use a tree to communicate. Due to faults or\nan asynchronous period, the tree may be deemed not robust\nand therefore a reconfiguration procedure is necessary to\nbuild a new tree. Naturally, not all possible trees are robust\nand several reconfigurations might be necessary before a\nrobust tree is found.Note that any leader-based protocol may require  + 1\nreconfigurations to find a robust topology, given that \nconsecutive leaders may be faulty. Our challenge is to avoid\nmaking the reconfiguration of Kauri superlinear with the\nnumber of processes, while also avoiding to fall back\nimmediately to a star topology as soon as a single fault occurs.We conjecture that building a general reconfiguration\nstrategy that yields a robust tree in a linear number of steps\nwithout falling back to a star topology is impossible, due\nto the large number of non-robust configurations that may\noccur in a tree. Consider, for instance, the case of binary trees\nwhere the number of possible binary trees is given by the\nCatalan number  = ((2+1))!! ! . From all these trees, only a\nsmall fraction is robust, namely those where faulty processes\nare not internal nodes (Definition 4). Thus, a reconfiguration\nstrategy that considers all possible configurations may\nrequire a factorial number of steps to find a robust tree. We\ndiscuss our reconfiguration strategy next.We model the sequence of trees as an evolving graph, i.e.\na sequence of static graphs (that are trees). To ensure that\neventually a robust tree is used, the evolving graph must\nobserve the following property:\nDefinition 5. Recurringly Robust Evolving Graph: An\nevolving graph G is said to be recurringly robust if robust\nstatic graphs appear infinitely often in its sequence.A recurringly robust evolving graph is suficient to ensure\nthat a robust graph will eventually be used by processes\nto communicate. However, this is undesirable in practice\nbecause the number of reconfigurations until a robust graph\nis found is unbounded. Because the system is essentially\nhalted during reconfiguration, we would like to find a robust\ngraph after a small number  of reconfigurations. We call this\nproperty of an evolving graph  -Bounded conformity.\nDefinition 6.  -Bounded Conformity: a recurringly robust\nevolving graph G exhibits  -Bounded conformity if a robust\nstatic graph appears in G at least once every  consecutive\nstatic graphs.We now introduce an algorithm that achieves  -Bounded\nConformity. We split all processes into  disjoint bins, each\nof size greater or equal to  , where  is the number of\ninternal nodes in a tree. Then we build an evolving graph by\ncreating trees whose internal nodes are drawn exclusively\nfrom a given bin, i.e., each tree   is built by picking a bin\n , following a round robin strategy, and by assigning nodes\nfrom bin  to all internal nodes of the tree (including the\nroot) as depicted in Algorithm 4.Theorem 3. Algorithm 4 constructs an evolving graph that\nsatisfies t-Bounded Conformity as long as  <  .Proof. Because the  bins are disjoint and there are at most\n <  faults, at least one of the bins is composed exclusively\nof correct nodes. In each  consecutive steps, the algorithm\npicks a tree whose internal nodes are drawn from distinct\nbins, hence guaranteeing that a robust tree is found. ?Algorithm 4 Construction of an Evolving Tree with\ntBounded Conformity\n1: Initially, split the set of processes N into  + 1 disjoint bins\n2: N ? 0 ? 1 ? . . . ?   ; s.t. | | ?  + 1 ?  ?  = ?.\n3: function build()\n654::: G??? pamlilcokpdoas(nsyib+tlre1et)erefreosmwhGose internal nodes are drawn exclusively from  .\n7: returnA limitation of the approach is that each bin must be large\nenough to contain at least as many processes as the number\nof internal nodes of a tree. This limits the number of bins\nthat can be generated and, therefore, the maximum value\nfor  . In a balanced tree of fanout  we can, at most, obtain\n disjoint bins with enough capacity to fill all the internal\npositions, such that this algorithm allows us to achieve at\nmost ( ? 1)-Bounded Conformity.\n5.3Given that Algorithm 4 can only achieve ( ? 1)-Bounded\nConformity, if  ?  we will not be able to reconfigure\nthe system in an optimal number of steps. Therefore, we\nadopt the following pragmatic approach to reconfiguration:\nwe execute Algorithm 4 and if after  steps a robust tree is\nnot found, Kauri falls back to a star. Thus, in the worst case\nscenario, Kauri performs  +  + 1 reconfigurations until a\nstar with a non-faulty leader is found.When the number of actual faults is small, i.e. if  <\n, Kauri recovers quickly without losing its scalability and\nthroughput properties. If the number of faults is large, i.e. if\n ?  <  /3, the system still recovers in a linear number of\nsteps, but falls back to the performance of the original (star\nbased) HotStuf protocol.", "5.1 Modeling Reconfiguration as an Evolving Graph": "We model the sequence of trees as an evolving graph, i.e.\na sequence of static graphs (that are trees). To ensure that\neventually a robust tree is used, the evolving graph must\nobserve the following property:\nDefinition 5. Recurringly Robust Evolving Graph: An\nevolving graph G is said to be recurringly robust if robust\nstatic graphs appear infinitely often in its sequence.A recurringly robust evolving graph is suficient to ensure\nthat a robust graph will eventually be used by processes\nto communicate. However, this is undesirable in practice\nbecause the number of reconfigurations until a robust graph\nis found is unbounded. Because the system is essentially\nhalted during reconfiguration, we would like to find a robust\ngraph after a small number  of reconfigurations. We call this\nproperty of an evolving graph  -Bounded conformity.\nDefinition 6.  -Bounded Conformity: a recurringly robust\nevolving graph G exhibits  -Bounded conformity if a robust\nstatic graph appears in G at least once every  consecutive\nstatic graphs.", "5.2 Achieving t-Bounded Conformity": "We now introduce an algorithm that achieves  -Bounded\nConformity. We split all processes into  disjoint bins, each\nof size greater or equal to  , where  is the number of\ninternal nodes in a tree. Then we build an evolving graph by\ncreating trees whose internal nodes are drawn exclusively\nfrom a given bin, i.e., each tree   is built by picking a bin\n , following a round robin strategy, and by assigning nodes\nfrom bin  to all internal nodes of the tree (including the\nroot) as depicted in Algorithm 4.Theorem 3. Algorithm 4 constructs an evolving graph that\nsatisfies t-Bounded Conformity as long as  <  .Proof. Because the  bins are disjoint and there are at most\n <  faults, at least one of the bins is composed exclusively\nof correct nodes. In each  consecutive steps, the algorithm\npicks a tree whose internal nodes are drawn from distinct\nbins, hence guaranteeing that a robust tree is found. ?Algorithm 4 Construction of an Evolving Tree with\ntBounded Conformity\n1: Initially, split the set of processes N into  + 1 disjoint bins\n2: N ? 0 ? 1 ? . . . ?   ; s.t. | | ?  + 1 ?  ?  = ?.\n3: function build()\n654::: G??? pamlilcokpdoas(nsyib+tlre1et)erefreosmwhGose internal nodes are drawn exclusively from  .\n7: returnA limitation of the approach is that each bin must be large\nenough to contain at least as many processes as the number\nof internal nodes of a tree. This limits the number of bins\nthat can be generated and, therefore, the maximum value\nfor  . In a balanced tree of fanout  we can, at most, obtain\n disjoint bins with enough capacity to fill all the internal\npositions, such that this algorithm allows us to achieve at\nmost ( ? 1)-Bounded Conformity.\n5.3", "8: end function": "A limitation of the approach is that each bin must be large\nenough to contain at least as many processes as the number\nof internal nodes of a tree. This limits the number of bins\nthat can be generated and, therefore, the maximum value\nfor  . In a balanced tree of fanout  we can, at most, obtain\n disjoint bins with enough capacity to fill all the internal\npositions, such that this algorithm allows us to achieve at\nmost ( ? 1)-Bounded Conformity.\n5.3", "Gracefully Degraded Reconfiguration": "Given that Algorithm 4 can only achieve ( ? 1)-Bounded\nConformity, if  ?  we will not be able to reconfigure\nthe system in an optimal number of steps. Therefore, we\nadopt the following pragmatic approach to reconfiguration:\nwe execute Algorithm 4 and if after  steps a robust tree is\nnot found, Kauri falls back to a star. Thus, in the worst case\nscenario, Kauri performs  +  + 1 reconfigurations until a\nstar with a non-faulty leader is found.When the number of actual faults is small, i.e. if  <\n, Kauri recovers quickly without losing its scalability and\nthroughput properties. If the number of faults is large, i.e. if\n ?  <  /3, the system still recovers in a linear number of\nsteps, but falls back to the performance of the original (star\nbased) HotStuf protocol.", "6 Implementation": "\n        We have implemented Kauri by extending the publicly\navailable HotStuf implementation 1. The core of the efort was\nextending the code to include the implementation of the\nbroadcastMsg and waitFor primitives, as specified in\nAlgorithm 2 and Algorithm 3, respectively. We also added support\nfor the bls cryptographic scheme by including the publicly\navailable implementation used in the Chia Blockchain [\n        \n        ].\nThis allows internal nodes to aggregate and verify the\nsignatures of their children, and thus balance the computational\nload. Both the verification cost of the signature aggregates,\nand the size of aggregates have a small O (1) complexity,\ncontributing to the overall eficiency of the implementation.\n      Because HotStuf and Kauri share the same codebase, we\nalso implemented a variant of HotStuf that uses the bls\nsignature scheme. As we discuss in more detail in the next\nsection, this allows us to assess the efects of our\ncontributions versus simply adopting another cryptographic scheme\n1Available at htps://github.com/hot-stuf/libhotstuf\nin HotStuf. We denote the original HotStuf implementation\nas HotStuf-secp and the bls variant as HotStuf-bls .\nPipelining: To implement pipelining, we use an estimation of\nthe parameters discussed in \u00a74 to compute the ideal time to\nstart the dissemination phase for the next consensus instance.\nIn the current implementation, we use a static pre-configured\nvalue but this could be automatically adapted at runtime,\nwhich we leave for future work.\n        Reconfiguration: To trigger reconfigurations we leverage the\nexisting HotStuf mechanisms. In detail, if no consensus is\nreached after a timeout, each process compiles a new-view\nmessage that includes the last successful quorum it observed\nand sends it to the next leader [\n        \n        ]. In turn, the candidate\nleader waits for 2 + 1 new-view messages and, depending on\nthe collected information, either continues the work of the\nprevious leader (if a block was previously locked) or proposes\nits own block (if no block had been locked yet). Similarly,\nin Kauri and upon timeout, each process invokes the build\nfunction of Algorithm 4 to construct the next tree and sends\nthe same new-view message to the root of the new tree. The\nroot then also awaits 2 + 1 new-view messages before\nreinitiating the protocol. Note that the new-view messages do\nnot use the tree and are instead sent directly to the candidate\nleader as in HotStuf. This is the only time in Kauri where\nall processes communicate directly with the leader.\nCode Availability: Overall, the implementation of the\nfunctionalities described above required the addition/adaptation\nof ? 1300 loc to the HotStuf codebase 2.\n7\n      ", "Evaluation": "In this section we evaluate Kauri across several scenarios.\n7.1\n          All experiments were performed on the Grid?5000 testbed [\n          \n          ].\nWe used 20 physical machines, each with two Intel Xeon\nE52620 v4 8-core CPUs and 64 GB RAM. We deploy a variable\nnumber of processes (from 100 to 400) in these machines. As\nwe will discuss later, in some configurations Kauri is able to\nsaturate the hardware resources of our testbed.\n        \n          We evaluate Kauri on a wide range of deployments, that\ncapture the diferent scenarios where we believe that\npermissioned blockchains with a large number of participants\nare likely to be used. More precisely, we consider the\nfollowing deployment scenarios: global, regional, and national. The\nglobal deployment models a globally distributed blockchain\nas used in other works [\n          \n          ] with 200 roundtrip\ntime (RTT) and 25/ bandwidth. The two other\nscenarios model reported industry use cases [\n          \n          ] in more limited\ngeographical deployments such as local supply-chain\nmanagement. The regional deployment captures a deployment in\na large country or unions of countries, such as the US or the\n2Available at htps://github.com/Raycoms/Kauri-Public\n        \n          EU, with 100 RTT and 100/ bandwidth. The national\ndeployment models a setting where nodes are closer to each\nother with 10 RTT and 1000/ bandwidth. Finally, we\nalso consider a heterogeneous deployment with a mix of\ndifferent bandwidth and RTT characteristics, as used in other\nrecent works[\n          \n          ]. We model the network characteristics of\neach scenario using NetEm[\n          \n          ].\n        \n          For most experiments, we use three system sizes with\n = 100, 200, 400 processes. As expected in most realistic\ndeployments, these values of  do not yield perfect -ary\ntrees. Therefore, we simply assign processes to tree nodes\nsuch that it approximates a balanced tree. Unless otherwise\nstated, this results in the following trees: for  = 100,  = 10\nfor the root and  = [\n          \n          ] for the other internal nodes; for\n = 200 the root?s fanout is  = 14, and for the other internal\nnodes  = [\n          \n          ], and for  = 400,  = 20 for the root and\n = [\n          \n          ] for the remaining internal nodes. This results in\ntrees with height 2 that are used throughout the experiments,\nunless otherwise stated.\n7.2\n        We now describe the values that are used to configure Kauri.\nTable 2 shows the values of the diferent parameters\nrequired to compute the Kauri pipelining stretch, following\nthe rationale introduced in \u00a74.3. We consider diferent\nbandwidth/RTT scenarios and blocks of 250 (plus signatures).\nFor each configuration we present the processing time, which\nwe have measured experimentally, the sending time, and the\nremaining time (\u00a74.3). These values are used to compute\nboth the target pipelining stretch and the expected\nmaximum speedup for each configuration. Note that, due to space\nconstraints, Table 2 does not include the parameter values\nfor all configurations we evaluate in the following sections.\nFor instance, we have also experimented with block sizes\ndiferent than 250. The purpose of the table is not to be\nexhaustive but to ofer a conceptual framework that makes\nit easier to reason about the experimental results presented\nin the next sections.\n7.3We start by showing the efects of the pipelining stretch on\nKauri?s throughput.Figure 5 depicts the throughput achieved when using Kauri\nin a setting with  = 100 in the global scenario (200\nRTT and 25/ bandwidth) for diferent block sizes and\nincreasing pipelining stretch values.For a blocksize of 250, the experimental numbers are\nclose to the numbers predicted by our model and presented\nin Table 2 (3rd line for Kauri), i.e. the best results are achieved\nwith a pipelining stretch close to 5. This shows that our\nperformance model, albeit simple, can ofer a good estimate of\nthe performance of the real system. The figure also shows\nthat, with smaller block sizes, higher pipelining stretch\nvalues are needed to make full use of the resources. This is also\no\ni\nr\na\nn\ne\ncS\nNational\nRegional\nGlobal\nGlobal\nGlobal\ntr(ouhhpguTK 2104356\n0\n2\n4\n6\n50Kb\n100Kb\n200Kb\n250Kb\n8 10 12 14 16 18 20Pipelining Stretch\nexpected given our model: with smaller block sizes the\nsending time is smaller and the idle portion of the remaining time\nis much larger. The ratio between these two values is also\nlarger, thus allowing Kauri to start more instances while it\nwaits for the responses from a previous instance. Naturally,\nit is more eficient to run fewer instances with more\ntransactions each than many instances with a small number of\ntransactions. For the rest of the evaluation, we use a blocksize\nof 250 for Kauri. For HotStuf, we empirically observed\nthat a blocksize of 250 also yielded the best results across\nthe diferent experiments.\n          We now compare the throughput of Kauri with HotStuf-secp,\nthe standard implementation, and our variant HotStuf-bls,\nin the national, regional and global scenarios. To assess the\nimpact of our pipelining scheme we also include results for\nKauri without pipelining (denoted as Kauri-np). By using\ntrees with BLS signatures but without pipelining, Kauri-np\ncaptures the performance characteristics of existing\nnonpipelining tree based systems such as Motor [\n          \n          ] and\nOmniledger [\n          \n          ]. The results are depicted in Figure 6.\n        The first observation is that, as expected, HotStuf is\nextremely sensitive to the available bandwidth and to the total\nnumber of nodes in the system. The larger the number of\nnodes, the longer it takes for the leader to finish a given\n100 150 200 250 300Processes\n(c) global (200 RTT - 25/ links)\n350\nround. Also, for a fixed value of  , the sending time increases\nsharply as the network bandwidth decreases. As a result,\nthe performance of HotStuf is highly penalized in systems\nwith large numbers of participants and limited bandwidth.\nAlso, since the use of bls signatures reduces bandwidth\nusage, HotStuf-bls performs better than HotStuf- secp in all\nscenarios, except when using the 1/ network.A second observation is that, without our pipelining\ntechniques, the performance of tree-based algorithms is mainly\nlimited by the RTT. This is illustrated by Kauri-np where\nthe throughput drops significantly when the RTT increases\nbut only drops slightly with the number of processes. It is\nalso interesting to observe that, as the network bandwidth\ndecreases, even without our pipelining, the use of a tree\nalready pays of. In fact, in the regional scenario, Kauri-np\nofers better throughput than HotStuf for a system with 200\nor more processes.Kauri, by leveraging the full capacity of the system via our\npipelining mechanism, outperforms both HotStuf variants\nand Kauri-np in all scenarios. Interestingly, despite the\nsimplifications adopted in our model (which, for instance,\nconsiders that computation and dissemination do not interfere\nwith each other) the predicted speedup over HotStuf-secp is\nvery close to the observed value. For instance, from Table 2\nKauri\nwe expected a speedup over HotStuf-secp of approximately\n? 30 for the system of 400 nodes in the global scenario and\nthe value obtained experimentally is 28.2 . This diference is\nquite reasonable given the number of simplifications adopted\nin the model. Overall, while the use of trees (Kauri-np) results\nin better performance than stars in certain scenarios, only\nthe combination of trees and pipelining permits to achieve a\nsubstantial performance increase.Note that the results of Kauri for the national scenario do\nnot match the predictions of our model, which predicts a\nspeedup of ? 6 with 400 processes. This is due to the\nlimitations of our testbed that does not have capacity to support\nthis experiment without incurring in a CPU bottleneck (note\nthat due to the limited number of physical servers we are\nforced to collocate several process per server). In the plot\nand the rest of the evaluation we highlight the data points\nobtained in a saturated testbed with a red circle.Finally, note that due to the use of a tree with a fixed height\nof ? = 2, Kauri?s throughput drops with the number of\nprocesses. In \u00a77.8 we show how throughput can be preserved by\nincreasing the height of the tree (and the pipelining stretch).Pipelining not only allows Kauri to better exploit the\navailable computing and network resources, but also contributes\nto mitigating the negative efects of additional RTT inherent\nin tree-based approaches. To further assess this, we\nconducted an experiment where we observe the throughput\nevolution as the RTT increases.Figure 7 shows the results for N=100 in the regional\nscenario (100/ bandwidth) but where we varied the RTT\nfrom 50 to 400. As it can be observed, while the\nthroughput of HotStuf-secp decreases as the RTT increases, the\nthroughput of Kauri can be kept almost constant by\nincreasing the pipelining stretch to avoid the leader from being\nidle while waiting for the replies. Following our model, the\npipelining stretch varied from 7, for an RTT of 50, to 33\nfor an RTT of 400. Results for the other scenarios (not\nshown due to space constraints) follow a similar trend.The previous experiments showed that Kauri, despite using\nlonger rounds, can achieve better throughput than HotStuf\n1400\n1200\n)s 1000\n(ycnem 860000\ntLa 240000\n0Kauri\nHotStuff-secpHotStuff-bls\n25\n50100 1000\nBandwidth (Mb/s)\n?\ndue to the use of the pipelining stretch. We now conduct a\nsimilar study on latency.Given that the use of a tree increases the latency required\nto exchange messages among the leader and the\nremaining processes, one could expect that Kauri would exhibit\nhigher system latency than HotStuf. In fact, in a system\nwith unlimited bandwidth and processing power, the latency\nof consensus is bound by the RTT. However, in realistic\nsettings, bandwidth is not infinite and in practice, the\nsystem latency is limited by the sending time. The\ndissemination/aggregation parallelism enabled by a tree substantially\nreduces the sending time. This is particularly important in\nbandwidth constrained scenarios where the sending time has\na much larger impact on latency than the additional number\nof communication hops required by a tree.To confirm this we set up a scenario with a fixed RTT of\n100 and vary the bandwidth from 25/ to 1000/.\nFigure 8 shows the results for  = 100. As shown, the\navailable bandwidth has a much larger impact on HotStuf-secp\nthan in Kauri. In fact, for bandwidths smaller than 100/\nKauri ofers better latency than Hotstuf-secp, and only at\nhigh bandwidths HotStuf-secp starts to have substantially\nbetter latency. The pictures also shows (analytical) values for\nan idealized scenario of infinite bandwidth, where\nHotStufsecp?s latency would be at best half of Kauri?s. As in \u00a77.5,\nresults for the other system sizes (not shown due to space\nconstraints) follow a similar trend.We now study the impact of load in the performance of the\nsystem. To this end we fix the system size, network\nbandwidth and latency, and vary the load in the system by\nmanipulating the block size, i.e. the number of transactions\nofered by the client. Results are depicted in Figure 9 for the\nglobal scenario with  = 100 and the following block sizes:\n32 64, 125, 250, 500, and 1. For Kauri we\nadjust the pipelining stretch for each scenario following our\nperformance model.Following the observations of the previous experiments,\nthe throughput of Kauri is substantially higher than that of\nboth variants of HotStuf in all scenarios. As the block size\nincreases, the latency of all systems increases as expected\ndue to an increase in the sending time. This increase is\nhowever much faster in both variants of HotStuf, and for block\n8192\n)s 4096\n(m 2048\ncy 1024\ntena 521526\nL 128\n100Kauri\nHotStuff-secp\nsizes larger than 125 HotStuf?s latency surpasses that\nof Kauri. This highlights the importance of using a tree to\nspread the load and hence avoid a bottleneck at the leader\nprocess. HotStuf-bls outperforms HotStuf-secp in both\nlatency and throughput for all scenarios, which also confirms\nthe previous experiments when varying the available\nbandwidth (Figure 8). Finally, the decrease in latency in Kauri\nwhen going from a block sizes of 32Kb to 64Kb (the two\nifrst data points in the Kauri line) is due to pipelining efects\non CPU usage. Blocks of 32Kb allows for a higher level of\npipelining, which saturates the CPU. As the block size\nincreases, the pipeline decreases, following our performance\nmodel (hence, gradually shifting the bottleneck away from\nthe CPU and to the network).The experiments of the previous sections have shown that\nthe sending time is a key factor for both throughput and\nlatency. By reducing the sending time, Kauri provides better\nthroughput in all considered scenarios and better latency in\nbandwidth constrained scenarios.We now study the impact of tree height on throughput\nand latency by setting an experiment with  = 100, an RTT\nof 100, and variable bandwidth. We deployed\nHotSufsecp, HotStuf-bls (with ? = 1 and  = 99), and two Kauri\nconfigurations: one with ? = 2 and  = 10, as in the previous\nexperiments, and another with ? = 3 and  = 5.Results are shown in Figure 10. The first observation is that\nby increasing the tree height it is possible to substantially\nincrease Kauri?s throughput, which almost doubles, with only\na modest impact on latency. The results for Kauri with ? = 2\nat 1/ bandwidth and ? = 3 at 100 and above saturate\nour testbed, which is denoted by the red circle. According\nto our model, for ? = 2 we expect the throughput to double\nwhen moving from a 100/ bandwidth to 1/. Similarly,\nfor ? = 3, we expect the throughput obtained with a\nbandwidth of 50/ to double and quadruple with a bandwidth\nof 100/ and 1/, respectively.The second observation corroborates the experiments of\n\u00a77.6 by showing that the latency of both variants of HotStuf\nvary substantially with the available bandwidth whereas\nKauri?s are much less afected. Finally, we can also see that\nHotStuf-bls can outperform HotStuf-secp in certain\nscenarios but only for a small margin as both systems share the\nsame sending time characteristics. To achieve a substantial\nimprovement one needs to drastically reduce the sending\ntime through the use of trees, and maximize resource usage\nthrough the use of pipelining.\n          So far we have only considered homogeneous networks,\nwhere all links have the same characteristics. We now\nconsider an heterogeneous setup, where participants are\ndeployed in local clusters (with very low RTT and high\nbandwidth within each cluster) connected over the Internet (with\nvarying bandwidth and RTT between clusters). For this\npurpose, we adopted the scenario used in the evaluation of\nResilientDB [\n          \n          ] in a system with  = 60. This allows us to\ncompare the performance of Kauri with the results published for\nResilientDB (we did not run ResilientDB ourselves because,\nat the time of this submission, the public implementation of\nResilientDB?s GeoBFT protocol was not yet available). When\ndeploying Kauri, we used a tree where the leader is placed\nin the cluster with the highest bandwidth and lowest RTT to\nevery other cluster (i.e. Oregon) and the internal nodes are\nlocated closely to their leaf nodes. Similarly, for the HotStuf\nvariants, the leader is located in Oregon.\n        Results are depicted in Figure 11. As can be observed,\nKauri?s throughput substantially outperforms all other\nsystems. This is due to the high RTT, which allows Kauri to\npipeline several consensus instances in parallel and hence\nachieve high throughput. In terms of latency, both HotStuf\nvariants outperform Kauri because the system size is small\nand hence bandwidth does not become a bottleneck.\nHowever, HotStuf latency would quickly grow with the system\nas bandwidth becomes the bottleneck, while Kauri?s latency\nwould remain largely unafected. Nonetheless, we stress that\nKauri achieves ? 10 higher throughput than HotStuf\nvariants with only a ? 2 increase in latency. Interestingly, the\nperformance of Kauri-np is the worse of all systems. This is\nbecause without pipelining the high RTT negatively afects\nthe remaining time, which ultimately limits throughput.\n          Note that the Hotstuf-secp throughput results we\nobtained are substantially lower than those reported in\nResilientDB. This is because ResilientDB?s authors [\n          \n          ] run\nKauri\n) 60\n/txs 50\n(K 40\ntpu 30\ngh 20\nu\nro 10\nhT 0\n        Kauri?np\n          HotStuff?bls\n independent HotStuf instances in parallel (where each\nprocess is the leader of one such instance) and account for\nthe overall throughput across all instances, which in practice\nequates to running  independent blockchains. We opted\ninstead to consider the throughput of a single instance\ndeployed in the best possible scenario. In this scenario, Kauri?s\nthroughput is on par with the reported throughput of\nResilientDB, which is ? 55 / [\n          \n          ]. However, ResilientDB\nperformance is dictated by the number of clusters while Kauri\nis independent of this factor. Moreover, ResilientDB tolerates\nonly  ? 3?1 , where  is the size of the smallest cluster\n(\u00a71), while Kauri tolerates  ? 3?1 faults as classical BFT.\n        It is worth noting that, in heterogeneous networks, all\nthese protocols require some amount of manual\nconfiguration for optimal performance (for instance, selecting the best\nplace for the leader and for internal nodes). Designing\nmethods to automatically find the best deployment configuration\nfor Kauri is left for future work.Finally, we evaluate the reconfiguration time in the following\nfaulty scenarios: i) one faulty leader, ii) three consecutive\nfaulty leaders, and iii)  faulty interior or leader nodes. In the\nifrst two cases,  < , thus Kauri keeps the tree topology,\nwhile in the third case,  is large and hence Kauri needs to\nfall back to a star. We empirically calibrate the fault detection\ntimeout, by starting with a large value and gradually\ndecreasing it until we found that further decreases would lead to\nspurious reconfigurations in a stable system. This resulted\nin a timeout of 0.35 for Kauri and 1.7 for HotStuf-secp.\nThis diference is explained by the fact that Kauri message\ndissemination is more regular than HotStuf, due to\npipelining, and hence the fault detection timeout can be set more\naggressively. Figure 12 presents the results for a system with\n = 100 in the global scenario. We execute each system for\n100 seconds (warm-up not shown), inject the fault at 40\nseconds and measure the impact on the throughput. As we can\nobserve, for a small number of faults both systems recover to\nthe throughput levels before the fault in ? 5 seconds for one\nfaulty leader (Figure 12a) and ? 12 seconds for three\nconsecutive faulty leaders (Figure 12b). As shown, Kauri recovers\nin the same time-frame as HotStuf.To assess the behavior of Kauri with a large number of\nfaulty nodes (i.e.  ) we set up two diferent scenarios depicted\nin Figure 12c. In the first scenario (plot line labeled ?Kauri\nleaders\")  leaders fail in succession. As expected after \nreconfigurations Kauri has fallen back to a star and stabilizes\nat around the same throughput of HotStuf. In the second\nscenario (plot line labeled ?Kauri internal+leaders\"), we start\nby failing internal nodes of  consecutive trees such that no\nconsensus can be reached for any of these trees. This forces\nKauri to degrade to a star after  reconfigurations. Then,\nwe selected faulty processes to serve as leaders for the first\n star networks. This is possible because a faulty process\ncan prevent consensus from being reached in two diferent\nconfigurations: if it is an internal node in a tree and if it is\nsubsequently picked as root node for a star. This scenario\nconstitutes the worst-case scenario for Kauri, causing  ++1\nreconfigurations, as stated in Section 5.3. In this scenario, the\ntotal recovery time is long because all configurations (except\nthe last) timeout. In detail, the timeout is initially set to 1.7,\ndoubled after each of the first two reconfigurations, and\nsubsequently capped at 10. Since  = 33 and  = 10, with\nHotStuf we need a total of 1.7 + 3.4 + 6.8 + 10 \u00b7 30 = 311.9\nto recover and with Kauri we need 10 \u00b7 10 = 100 more.\nNonetheless, after the system stabilizes the performance of\nKauri is at the same level as HotStuf.", "Experimental Setup": "\n          All experiments were performed on the Grid?5000 testbed [\n          \n          ].\nWe used 20 physical machines, each with two Intel Xeon\nE52620 v4 8-core CPUs and 64 GB RAM. We deploy a variable\nnumber of processes (from 100 to 400) in these machines. As\nwe will discuss later, in some configurations Kauri is able to\nsaturate the hardware resources of our testbed.\n        \n          We evaluate Kauri on a wide range of deployments, that\ncapture the diferent scenarios where we believe that\npermissioned blockchains with a large number of participants\nare likely to be used. More precisely, we consider the\nfollowing deployment scenarios: global, regional, and national. The\nglobal deployment models a globally distributed blockchain\nas used in other works [\n          \n          ] with 200 roundtrip\ntime (RTT) and 25/ bandwidth. The two other\nscenarios model reported industry use cases [\n          \n          ] in more limited\ngeographical deployments such as local supply-chain\nmanagement. The regional deployment captures a deployment in\na large country or unions of countries, such as the US or the\n2Available at htps://github.com/Raycoms/Kauri-Public\n        \n          EU, with 100 RTT and 100/ bandwidth. The national\ndeployment models a setting where nodes are closer to each\nother with 10 RTT and 1000/ bandwidth. Finally, we\nalso consider a heterogeneous deployment with a mix of\ndifferent bandwidth and RTT characteristics, as used in other\nrecent works[\n          \n          ]. We model the network characteristics of\neach scenario using NetEm[\n          \n          ].\n        \n          For most experiments, we use three system sizes with\n = 100, 200, 400 processes. As expected in most realistic\ndeployments, these values of  do not yield perfect -ary\ntrees. Therefore, we simply assign processes to tree nodes\nsuch that it approximates a balanced tree. Unless otherwise\nstated, this results in the following trees: for  = 100,  = 10\nfor the root and  = [\n          \n          ] for the other internal nodes; for\n = 200 the root?s fanout is  = 14, and for the other internal\nnodes  = [\n          \n          ], and for  = 400,  = 20 for the root and\n = [\n          \n          ] for the remaining internal nodes. This results in\ntrees with height 2 that are used throughout the experiments,\nunless otherwise stated.\n7.2\n        ", "Configuring Kauri": "We now describe the values that are used to configure Kauri.\nTable 2 shows the values of the diferent parameters\nrequired to compute the Kauri pipelining stretch, following\nthe rationale introduced in \u00a74.3. We consider diferent\nbandwidth/RTT scenarios and blocks of 250 (plus signatures).\nFor each configuration we present the processing time, which\nwe have measured experimentally, the sending time, and the\nremaining time (\u00a74.3). These values are used to compute\nboth the target pipelining stretch and the expected\nmaximum speedup for each configuration. Note that, due to space\nconstraints, Table 2 does not include the parameter values\nfor all configurations we evaluate in the following sections.\nFor instance, we have also experimented with block sizes\ndiferent than 250. The purpose of the table is not to be\nexhaustive but to ofer a conceptual framework that makes\nit easier to reason about the experimental results presented\nin the next sections.\n7.3", "Efect of Pipelining Stretch on Throughput": "We start by showing the efects of the pipelining stretch on\nKauri?s throughput.Figure 5 depicts the throughput achieved when using Kauri\nin a setting with  = 100 in the global scenario (200\nRTT and 25/ bandwidth) for diferent block sizes and\nincreasing pipelining stretch values.For a blocksize of 250, the experimental numbers are\nclose to the numbers predicted by our model and presented\nin Table 2 (3rd line for Kauri), i.e. the best results are achieved\nwith a pipelining stretch close to 5. This shows that our\nperformance model, albeit simple, can ofer a good estimate of\nthe performance of the real system. The figure also shows\nthat, with smaller block sizes, higher pipelining stretch\nvalues are needed to make full use of the resources. This is also\no\ni\nr\na\nn\ne\ncS\nNational\nRegional\nGlobal\nGlobal\nGlobal\ntr(ouhhpguTK 2104356\n0\n2\n4\n6\n50Kb\n100Kb\n200Kb\n250Kb\n8 10 12 14 16 18 20Pipelining Stretch\nexpected given our model: with smaller block sizes the\nsending time is smaller and the idle portion of the remaining time\nis much larger. The ratio between these two values is also\nlarger, thus allowing Kauri to start more instances while it\nwaits for the responses from a previous instance. Naturally,\nit is more eficient to run fewer instances with more\ntransactions each than many instances with a small number of\ntransactions. For the rest of the evaluation, we use a blocksize\nof 250 for Kauri. For HotStuf, we empirically observed\nthat a blocksize of 250 also yielded the best results across\nthe diferent experiments.", "7.4 Throughput Across Diferent Scenarios": "\n          We now compare the throughput of Kauri with HotStuf-secp,\nthe standard implementation, and our variant HotStuf-bls,\nin the national, regional and global scenarios. To assess the\nimpact of our pipelining scheme we also include results for\nKauri without pipelining (denoted as Kauri-np). By using\ntrees with BLS signatures but without pipelining, Kauri-np\ncaptures the performance characteristics of existing\nnonpipelining tree based systems such as Motor [\n          \n          ] and\nOmniledger [\n          \n          ]. The results are depicted in Figure 6.\n        The first observation is that, as expected, HotStuf is\nextremely sensitive to the available bandwidth and to the total\nnumber of nodes in the system. The larger the number of\nnodes, the longer it takes for the leader to finish a given\n100 150 200 250 300Processes\n(c) global (200 RTT - 25/ links)\n350\nround. Also, for a fixed value of  , the sending time increases\nsharply as the network bandwidth decreases. As a result,\nthe performance of HotStuf is highly penalized in systems\nwith large numbers of participants and limited bandwidth.\nAlso, since the use of bls signatures reduces bandwidth\nusage, HotStuf-bls performs better than HotStuf- secp in all\nscenarios, except when using the 1/ network.A second observation is that, without our pipelining\ntechniques, the performance of tree-based algorithms is mainly\nlimited by the RTT. This is illustrated by Kauri-np where\nthe throughput drops significantly when the RTT increases\nbut only drops slightly with the number of processes. It is\nalso interesting to observe that, as the network bandwidth\ndecreases, even without our pipelining, the use of a tree\nalready pays of. In fact, in the regional scenario, Kauri-np\nofers better throughput than HotStuf for a system with 200\nor more processes.Kauri, by leveraging the full capacity of the system via our\npipelining mechanism, outperforms both HotStuf variants\nand Kauri-np in all scenarios. Interestingly, despite the\nsimplifications adopted in our model (which, for instance,\nconsiders that computation and dissemination do not interfere\nwith each other) the predicted speedup over HotStuf-secp is\nvery close to the observed value. For instance, from Table 2\nKauri\nwe expected a speedup over HotStuf-secp of approximately\n? 30 for the system of 400 nodes in the global scenario and\nthe value obtained experimentally is 28.2 . This diference is\nquite reasonable given the number of simplifications adopted\nin the model. Overall, while the use of trees (Kauri-np) results\nin better performance than stars in certain scenarios, only\nthe combination of trees and pipelining permits to achieve a\nsubstantial performance increase.Note that the results of Kauri for the national scenario do\nnot match the predictions of our model, which predicts a\nspeedup of ? 6 with 400 processes. This is due to the\nlimitations of our testbed that does not have capacity to support\nthis experiment without incurring in a CPU bottleneck (note\nthat due to the limited number of physical servers we are\nforced to collocate several process per server). In the plot\nand the rest of the evaluation we highlight the data points\nobtained in a saturated testbed with a red circle.Finally, note that due to the use of a tree with a fixed height\nof ? = 2, Kauri?s throughput drops with the number of\nprocesses. In \u00a77.8 we show how throughput can be preserved by\nincreasing the height of the tree (and the pipelining stretch).", "7.5 Efect of the RTT in Throughput": "Pipelining not only allows Kauri to better exploit the\navailable computing and network resources, but also contributes\nto mitigating the negative efects of additional RTT inherent\nin tree-based approaches. To further assess this, we\nconducted an experiment where we observe the throughput\nevolution as the RTT increases.Figure 7 shows the results for N=100 in the regional\nscenario (100/ bandwidth) but where we varied the RTT\nfrom 50 to 400. As it can be observed, while the\nthroughput of HotStuf-secp decreases as the RTT increases, the\nthroughput of Kauri can be kept almost constant by\nincreasing the pipelining stretch to avoid the leader from being\nidle while waiting for the replies. Following our model, the\npipelining stretch varied from 7, for an RTT of 50, to 33\nfor an RTT of 400. Results for the other scenarios (not\nshown due to space constraints) follow a similar trend.", "7.6 Latency": "The previous experiments showed that Kauri, despite using\nlonger rounds, can achieve better throughput than HotStuf\n1400\n1200\n)s 1000\n(ycnem 860000\ntLa 240000\n0Kauri\nHotStuff-secpHotStuff-bls\n25\n50100 1000\nBandwidth (Mb/s)\n?\ndue to the use of the pipelining stretch. We now conduct a\nsimilar study on latency.Given that the use of a tree increases the latency required\nto exchange messages among the leader and the\nremaining processes, one could expect that Kauri would exhibit\nhigher system latency than HotStuf. In fact, in a system\nwith unlimited bandwidth and processing power, the latency\nof consensus is bound by the RTT. However, in realistic\nsettings, bandwidth is not infinite and in practice, the\nsystem latency is limited by the sending time. The\ndissemination/aggregation parallelism enabled by a tree substantially\nreduces the sending time. This is particularly important in\nbandwidth constrained scenarios where the sending time has\na much larger impact on latency than the additional number\nof communication hops required by a tree.To confirm this we set up a scenario with a fixed RTT of\n100 and vary the bandwidth from 25/ to 1000/.\nFigure 8 shows the results for  = 100. As shown, the\navailable bandwidth has a much larger impact on HotStuf-secp\nthan in Kauri. In fact, for bandwidths smaller than 100/\nKauri ofers better latency than Hotstuf-secp, and only at\nhigh bandwidths HotStuf-secp starts to have substantially\nbetter latency. The pictures also shows (analytical) values for\nan idealized scenario of infinite bandwidth, where\nHotStufsecp?s latency would be at best half of Kauri?s. As in \u00a77.5,\nresults for the other system sizes (not shown due to space\nconstraints) follow a similar trend.", "7.7 Throughput vs Latency": "We now study the impact of load in the performance of the\nsystem. To this end we fix the system size, network\nbandwidth and latency, and vary the load in the system by\nmanipulating the block size, i.e. the number of transactions\nofered by the client. Results are depicted in Figure 9 for the\nglobal scenario with  = 100 and the following block sizes:\n32 64, 125, 250, 500, and 1. For Kauri we\nadjust the pipelining stretch for each scenario following our\nperformance model.Following the observations of the previous experiments,\nthe throughput of Kauri is substantially higher than that of\nboth variants of HotStuf in all scenarios. As the block size\nincreases, the latency of all systems increases as expected\ndue to an increase in the sending time. This increase is\nhowever much faster in both variants of HotStuf, and for block\n8192\n)s 4096\n(m 2048\ncy 1024\ntena 521526\nL 128\n100Kauri\nHotStuff-secp\nsizes larger than 125 HotStuf?s latency surpasses that\nof Kauri. This highlights the importance of using a tree to\nspread the load and hence avoid a bottleneck at the leader\nprocess. HotStuf-bls outperforms HotStuf-secp in both\nlatency and throughput for all scenarios, which also confirms\nthe previous experiments when varying the available\nbandwidth (Figure 8). Finally, the decrease in latency in Kauri\nwhen going from a block sizes of 32Kb to 64Kb (the two\nifrst data points in the Kauri line) is due to pipelining efects\non CPU usage. Blocks of 32Kb allows for a higher level of\npipelining, which saturates the CPU. As the block size\nincreases, the pipeline decreases, following our performance\nmodel (hence, gradually shifting the bottleneck away from\nthe CPU and to the network).", "7.8 Impact of Tree Height in Throughput/ Latency": "The experiments of the previous sections have shown that\nthe sending time is a key factor for both throughput and\nlatency. By reducing the sending time, Kauri provides better\nthroughput in all considered scenarios and better latency in\nbandwidth constrained scenarios.We now study the impact of tree height on throughput\nand latency by setting an experiment with  = 100, an RTT\nof 100, and variable bandwidth. We deployed\nHotSufsecp, HotStuf-bls (with ? = 1 and  = 99), and two Kauri\nconfigurations: one with ? = 2 and  = 10, as in the previous\nexperiments, and another with ? = 3 and  = 5.Results are shown in Figure 10. The first observation is that\nby increasing the tree height it is possible to substantially\nincrease Kauri?s throughput, which almost doubles, with only\na modest impact on latency. The results for Kauri with ? = 2\nat 1/ bandwidth and ? = 3 at 100 and above saturate\nour testbed, which is denoted by the red circle. According\nto our model, for ? = 2 we expect the throughput to double\nwhen moving from a 100/ bandwidth to 1/. Similarly,\nfor ? = 3, we expect the throughput obtained with a\nbandwidth of 50/ to double and quadruple with a bandwidth\nof 100/ and 1/, respectively.The second observation corroborates the experiments of\n\u00a77.6 by showing that the latency of both variants of HotStuf\nvary substantially with the available bandwidth whereas\nKauri?s are much less afected. Finally, we can also see that\nHotStuf-bls can outperform HotStuf-secp in certain\nscenarios but only for a small margin as both systems share the\nsame sending time characteristics. To achieve a substantial\nimprovement one needs to drastically reduce the sending\ntime through the use of trees, and maximize resource usage\nthrough the use of pipelining.", "7.9 Heterogeneous Networks": "\n          So far we have only considered homogeneous networks,\nwhere all links have the same characteristics. We now\nconsider an heterogeneous setup, where participants are\ndeployed in local clusters (with very low RTT and high\nbandwidth within each cluster) connected over the Internet (with\nvarying bandwidth and RTT between clusters). For this\npurpose, we adopted the scenario used in the evaluation of\nResilientDB [\n          \n          ] in a system with  = 60. This allows us to\ncompare the performance of Kauri with the results published for\nResilientDB (we did not run ResilientDB ourselves because,\nat the time of this submission, the public implementation of\nResilientDB?s GeoBFT protocol was not yet available). When\ndeploying Kauri, we used a tree where the leader is placed\nin the cluster with the highest bandwidth and lowest RTT to\nevery other cluster (i.e. Oregon) and the internal nodes are\nlocated closely to their leaf nodes. Similarly, for the HotStuf\nvariants, the leader is located in Oregon.\n        Results are depicted in Figure 11. As can be observed,\nKauri?s throughput substantially outperforms all other\nsystems. This is due to the high RTT, which allows Kauri to\npipeline several consensus instances in parallel and hence\nachieve high throughput. In terms of latency, both HotStuf\nvariants outperform Kauri because the system size is small\nand hence bandwidth does not become a bottleneck.\nHowever, HotStuf latency would quickly grow with the system\nas bandwidth becomes the bottleneck, while Kauri?s latency\nwould remain largely unafected. Nonetheless, we stress that\nKauri achieves ? 10 higher throughput than HotStuf\nvariants with only a ? 2 increase in latency. Interestingly, the\nperformance of Kauri-np is the worse of all systems. This is\nbecause without pipelining the high RTT negatively afects\nthe remaining time, which ultimately limits throughput.\n          Note that the Hotstuf-secp throughput results we\nobtained are substantially lower than those reported in\nResilientDB. This is because ResilientDB?s authors [\n          \n          ] run\nKauri\n) 60\n/txs 50\n(K 40\ntpu 30\ngh 20\nu\nro 10\nhT 0\n        Kauri?np\n          HotStuff?bls\n independent HotStuf instances in parallel (where each\nprocess is the leader of one such instance) and account for\nthe overall throughput across all instances, which in practice\nequates to running  independent blockchains. We opted\ninstead to consider the throughput of a single instance\ndeployed in the best possible scenario. In this scenario, Kauri?s\nthroughput is on par with the reported throughput of\nResilientDB, which is ? 55 / [\n          \n          ]. However, ResilientDB\nperformance is dictated by the number of clusters while Kauri\nis independent of this factor. Moreover, ResilientDB tolerates\nonly  ? 3?1 , where  is the size of the smallest cluster\n(\u00a71), while Kauri tolerates  ? 3?1 faults as classical BFT.\n        It is worth noting that, in heterogeneous networks, all\nthese protocols require some amount of manual\nconfiguration for optimal performance (for instance, selecting the best\nplace for the leader and for internal nodes). Designing\nmethods to automatically find the best deployment configuration\nfor Kauri is left for future work.", "7.10 Reconfiguration": "Finally, we evaluate the reconfiguration time in the following\nfaulty scenarios: i) one faulty leader, ii) three consecutive\nfaulty leaders, and iii)  faulty interior or leader nodes. In the\nifrst two cases,  < , thus Kauri keeps the tree topology,\nwhile in the third case,  is large and hence Kauri needs to\nfall back to a star. We empirically calibrate the fault detection\ntimeout, by starting with a large value and gradually\ndecreasing it until we found that further decreases would lead to\nspurious reconfigurations in a stable system. This resulted\nin a timeout of 0.35 for Kauri and 1.7 for HotStuf-secp.\nThis diference is explained by the fact that Kauri message\ndissemination is more regular than HotStuf, due to\npipelining, and hence the fault detection timeout can be set more\naggressively. Figure 12 presents the results for a system with\n = 100 in the global scenario. We execute each system for\n100 seconds (warm-up not shown), inject the fault at 40\nseconds and measure the impact on the throughput. As we can\nobserve, for a small number of faults both systems recover to\nthe throughput levels before the fault in ? 5 seconds for one\nfaulty leader (Figure 12a) and ? 12 seconds for three\nconsecutive faulty leaders (Figure 12b). As shown, Kauri recovers\nin the same time-frame as HotStuf.To assess the behavior of Kauri with a large number of\nfaulty nodes (i.e.  ) we set up two diferent scenarios depicted\nin Figure 12c. In the first scenario (plot line labeled ?Kauri\nleaders\")  leaders fail in succession. As expected after \nreconfigurations Kauri has fallen back to a star and stabilizes\nat around the same throughput of HotStuf. In the second\nscenario (plot line labeled ?Kauri internal+leaders\"), we start\nby failing internal nodes of  consecutive trees such that no\nconsensus can be reached for any of these trees. This forces\nKauri to degrade to a star after  reconfigurations. Then,\nwe selected faulty processes to serve as leaders for the first\n star networks. This is possible because a faulty process\ncan prevent consensus from being reached in two diferent\nconfigurations: if it is an internal node in a tree and if it is\nsubsequently picked as root node for a star. This scenario\nconstitutes the worst-case scenario for Kauri, causing  ++1\nreconfigurations, as stated in Section 5.3. In this scenario, the\ntotal recovery time is long because all configurations (except\nthe last) timeout. In detail, the timeout is initially set to 1.7,\ndoubled after each of the first two reconfigurations, and\nsubsequently capped at 10. Since  = 33 and  = 10, with\nHotStuf we need a total of 1.7 + 3.4 + 6.8 + 10 \u00b7 30 = 311.9\nto recover and with Kauri we need 10 \u00b7 10 = 100 more.\nNonetheless, after the system stabilizes the performance of\nKauri is at the same level as HotStuf.", "8 Conclusions and Future Work": "State-of-the-art permissioned blockchains sufer from\nimportant limitations: bottlenecks resulting from work\nconcentration on the leader, throughput decrease when\nimplementing load distribution, or degraded resilience. Kauri\novercomes these limitations by introducing a novel pipelining\nscheme that makes full use of the parallelization\nopportunities provided by dissemination/ aggregation trees.\nFurthermore, Kauri uses a reconfiguration strategy that preserves\nthe tree when the number of faults is smaller than the fanout,\nwhile still ensuring that a robust configuration is found in\na linear number of steps for any number of faults  <  /3.\nIn contrast with solutions based on committees, Kauri does\nnot compromise the resilience nor the finality of consensus.\nKauri?s throughput substantially outperforms HotStuf?s in\nall considered scenarios, reaching up to 28x with only a\nmodest increase in latency. In bandwidth-constrained scenarios\nKauri outperforms HotStuf in both throughput and latency.\nThe current implementation of Kauri requires the topology of\nthe tree and the value of the pipelining stretch to be manually\nconfigured, using the performance model provided in this\npaper. We are currently working on algorithms to automate\nand optimize these configurations.Acknowledgements: We thank our shepherd, Andreas Haeberlen, and\nthe anonymous reviewers for their help in improving the final version\nof the manuscript. This work was partially supported by CAPES - Brazil\n(Coordena\u00e7\u00e3o de Aperfei\u00e7oamento de Pessoal de N\u00edvel Superior) and by\nFunda\u00e7\u00e3o para a Ci\u00eancia e Tecnologia (FCT) under project UIDB/50021/2020\nand grant 2020.05270.BD, and via project COSMOS (via the OE with ref.\n) 10\nt/xs 89\nt(uK 567\nhpgou 432\nrh 1\nT 0\n) 10\n/txs 89\nt(uK 675\n0\n10\n20\n30\n40\n60\n70\n80PTDC/EEI-COM/29271/2017 and via the ?Programa Operacional Regional de\nLisboa na sua componente FEDER? with ref. Lisboa-01-0145-FEDER-029271)\nand project Angainor with reference LISBOA-01-0145-FEDER-031456."}